---
phase: 08-residual-rl-and-learning
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - evcc-smartload/rootfs/app/rl_agent.py
  - evcc-smartload/rootfs/app/comparator.py
autonomous: true
requirements: [LERN-01]

must_haves:
  truths:
    - "RL agent outputs signed delta corrections (+/-20ct) on the planner's battery and EV price thresholds, never full actions"
    - "Stratified replay buffer retains samples from all four seasons equally"
    - "Old Q-table is detected as incompatible and cleanly reset (model_version mismatch)"
    - "Comparator tracks plan_cost vs actual_cost using slot-0 energy accounting, not full-horizon LP objective"
  artifacts:
    - path: "evcc-smartload/rootfs/app/rl_agent.py"
      provides: "ResidualRLAgent class with delta correction output, shadow/advisory mode, stratified replay"
      contains: "class ResidualRLAgent"
    - path: "evcc-smartload/rootfs/app/comparator.py"
      provides: "Extended Comparator with slot-0 cost comparison and rolling 7-day window"
      contains: "plan_slot0_cost"
  key_links:
    - from: "evcc-smartload/rootfs/app/rl_agent.py"
      to: "state.SystemState"
      via: "select_delta(state) uses state.to_vector() for Q-table lookup"
      pattern: "select_delta.*state.*to_vector"
    - from: "evcc-smartload/rootfs/app/rl_agent.py"
      to: "/data/smartprice_rl_model.json"
      via: "save/load with model_version=2 for clean migration"
      pattern: "model_version.*2"
---

<objective>
Refactor the existing DQNAgent into a ResidualRLAgent that outputs signed delta corrections (+/-20ct) on the LP planner's battery and EV price thresholds instead of selecting full actions. Extend the Comparator to use slot-0 energy cost accounting.

Purpose: LERN-01 requires the RL agent to learn corrections to the planner rather than making independent decisions. This eliminates conflict between RL and the LP safety guarantees.
Output: ResidualRLAgent class in rl_agent.py, extended Comparator in comparator.py
</objective>

<execution_context>
@C:/Users/nicok/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/nicok/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-residual-rl-and-learning/08-RESEARCH.md
@evcc-smartload/rootfs/app/rl_agent.py
@evcc-smartload/rootfs/app/comparator.py
@evcc-smartload/rootfs/app/state.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace DQNAgent with ResidualRLAgent class</name>
  <files>evcc-smartload/rootfs/app/rl_agent.py</files>
  <action>
Refactor rl_agent.py to replace the DQNAgent class with ResidualRLAgent. Keep the file path and ReplayMemory class. The new agent:

1. **Action space:** 7 battery deltas x 7 EV deltas = 49 total actions. Delta options: [-20, -10, -5, 0, +5, +10, +20] ct for both battery and EV thresholds. Constants: `DELTA_OPTIONS_CT`, `N_BAT_DELTAS=7`, `N_EV_DELTAS=7`, `N_ACTIONS=49`.

2. **State vector:** Reuse existing `state.to_vector()` (31 features). `STATE_SIZE=31` unchanged. The Q-table incompatibility comes from N_ACTIONS changing (35->49), not state size.

3. **select_delta(state, explore=True) -> tuple[float, float]:** Returns `(bat_delta_ct, ev_delta_ct)`. Epsilon-greedy selection: random action with probability epsilon, else argmax Q-table. Decompose action_idx: `bat_idx = action_idx // N_EV_DELTAS`, `ev_idx = action_idx % N_EV_DELTAS`.

4. **apply_correction(plan_bat_price_ct, plan_ev_price_ct, delta_bat_ct, delta_ev_ct, state) -> tuple[float, float]:** Returns `(adj_bat_ct, adj_ev_ct)`. Apply delta, clip to `[0, plan_price + DELTA_CLIP_CT]` where `DELTA_CLIP_CT=20.0`. Both adjusted prices must be >= 0.

5. **calculate_reward(plan_cost_eur, actual_cost_eur) -> float:** Returns `plan_cost_eur - actual_cost_eur` (positive = RL improved on plan).

6. **Stratified replay buffer:** Replace single ReplayMemory with `StratifiedReplayBuffer`. Four sub-buffers (one per season: winter/spring/summer/autumn), each `capacity // 4` deque. Season detection via explicit MONTH_TO_SEASON mapping: `{12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}` (Pitfall 3 from research). Sampling draws equally from all non-empty season sub-buffers.

7. **Shadow/advisory mode:** `self.mode` attribute: `"shadow"` or `"advisory"`. `self.shadow_start_timestamp` (UTC datetime). `shadow_elapsed_days() -> int`. Methods: `log_shadow_correction(bat_delta, ev_delta, plan_bat_price_ct, plan_ev_price_ct, state)` stores shadow episodes in a list for constraint audit. Skip logging when override is active (caller responsibility, but provide `is_override_active` param).

8. **Persistence:** Save to `/data/smartprice_rl_model.json` with `"model_version": 2`. On load(), check version -- if mismatch (old model is version 1 or missing), reset Q-table and log warning. Use atomic write pattern (write .tmp, os.replace). Also save/load shadow_start_timestamp, mode, shadow_corrections list.

9. **Shadow log persistence:** `/data/smartprice_rl_shadow_log.json` for shadow corrections (separate file to avoid bloating main model file). Save periodically (every 50 corrections).

10. **Learning:** Keep Q-learning update: `Q[s,a] += alpha * (reward + gamma * max(Q[s',a']) - Q[s,a])`. Use existing epsilon decay pattern. Alpha=0.1, gamma=0.95 (same as existing DQNAgent). Add `learn_from_correction(state, action_idx, reward, next_state)` method.

11. **Deprecate old DQNAgent:** Keep the class as `_DeprecatedDQNAgent` (renamed, not deleted) for reference, but it should not be imported or used by main.py. Comment at top: "Deprecated: replaced by ResidualRLAgent in Phase 8".

12. **run_constraint_audit() -> dict:** Run the 4-item constraint audit checklist (as specified in RESEARCH.md): (a) battery min_soc never violated during shadow corrections, (b) departure target never missed, (c) all deltas stayed within DELTA_CLIP_CT range, (d) win-rate > 50% over shadow period. Returns `{"checks": [{"name": str, "passed": bool, "detail": str}, ...], "all_passed": bool}`. Uses `self._shadow_corrections` list to evaluate constraints.

13. **get_audit_result() -> Optional[dict]:** Returns the result of the most recent `run_constraint_audit()` call, or None if no audit has been run yet. Store as `self._last_audit_result`. Also used by `maybe_promote(audit_result)` which promotes mode from `"shadow"` to `"advisory"` if `audit_result["all_passed"]` is True.

Do NOT modify `state.py` -- the state vector remains 31 features.
Do NOT extend the state vector with seasonal learner data (research recommendation: keep STATE_SIZE=31).
  </action>
  <verify>
    <automated>cd /c/users/nicok/projects/smartload && python -c "
import sys; sys.path.insert(0, 'evcc-smartload/rootfs/app')
# Mock numpy if needed
try:
    import numpy
except ImportError:
    import types; np = types.ModuleType('numpy'); np.zeros = lambda s: [0]*s if isinstance(s,int) else [[0]*s[1] for _ in range(s[0])]; np.clip = lambda v,lo,hi: max(lo,min(hi,v)); np.argmax = lambda a: a.index(max(a)); sys.modules['numpy'] = np
from rl_agent import ResidualRLAgent, DELTA_OPTIONS_CT, N_ACTIONS, StratifiedReplayBuffer
assert N_ACTIONS == 49, f'Expected 49 actions, got {N_ACTIONS}'
assert len(DELTA_OPTIONS_CT) == 7
assert DELTA_OPTIONS_CT == [-20, -10, -5, 0, 5, 10, 20]
agent = ResidualRLAgent.__new__(ResidualRLAgent)
assert hasattr(agent, 'select_delta')
assert hasattr(agent, 'apply_correction')
assert hasattr(agent, 'calculate_reward')
assert hasattr(agent, 'log_shadow_correction')
assert hasattr(agent, 'mode')
assert hasattr(agent, 'run_constraint_audit')
assert hasattr(agent, 'get_audit_result')
assert hasattr(agent, 'maybe_promote')
print('PASS: ResidualRLAgent structure verified')
"
    </automated>
    <manual>Verify rl_agent.py has ResidualRLAgent class with 49-action delta space</manual>
  </verify>
  <done>ResidualRLAgent class exists with select_delta() returning (bat_delta_ct, ev_delta_ct), apply_correction() enforcing safety constraints, stratified replay buffer with 4 seasonal sub-buffers, shadow/advisory mode with persistence, model_version=2 migration guard, run_constraint_audit() returning 4-check audit dict, get_audit_result() returning cached audit, and maybe_promote() for shadow-to-advisory transition</done>
</task>

<task type="auto">
  <name>Task 2: Extend Comparator for slot-0 cost accounting and rolling windows</name>
  <files>evcc-smartload/rootfs/app/comparator.py</files>
  <action>
Extend the existing Comparator class to support ResidualRLAgent delta corrections:

1. **New method: `compare_residual(plan_slot0_cost_eur, actual_slot0_cost_eur, delta_bat_ct, delta_ev_ct)`:** Records a comparison using slot-0 energy cost (not full-horizon LP objective). Stores in existing comparison list. `rl_better = actual_slot0_cost_eur < plan_slot0_cost_eur` (RL correction resulted in lower actual cost).

   **CRITICAL (Pitfall 2):** `plan_slot0_cost_eur` must be the slot-0 grid energy cost: `slot0.price_eur_kwh * (slot0.bat_charge_kw + slot0.ev_charge_kw) * 0.25` (15-min slot energy). Do NOT use `plan.solver_fun` (that is the full 24h cost). Document this in a docstring.

2. **Rolling 7-day window:** Add `get_recent_comparisons(days=7) -> list[dict]` that filters comparison list to entries within the last `days` days. Each entry: `{"timestamp": iso_str, "plan_cost_eur": float, "actual_cost_eur": float, "rl_better": bool, "delta_bat_ct": float, "delta_ev_ct": float}`.

3. **Cumulative savings:** Add `cumulative_savings_eur() -> float` returning sum of `(plan_cost - actual_cost)` over all recorded comparisons. Prefix with "ca." in dashboard (caller responsibility).

4. **avg_daily_savings() -> Optional[float]:** Average daily savings over the last 7 days. Returns None if fewer than 1 day of data exists.

5. **Backward compatibility:** Keep existing `compare()` method and `get_status()` dict structure for the existing Status tab dashboard. The existing `/comparisons` endpoint must continue working. New methods are additive, not replacing.

6. **Persistence:** Extend existing JSON save/load to include residual comparison entries alongside existing data. Use same file path `/data/smartprice_comparison.json`. Add `"version": 2` field to distinguish from old format; old format loaded with graceful fallback (reset residual entries, keep legacy data).
  </action>
  <verify>
    <automated>cd /c/users/nicok/projects/smartload && python -c "
import sys; sys.path.insert(0, 'evcc-smartload/rootfs/app')
try:
    import numpy
except ImportError:
    import types; np = types.ModuleType('numpy'); np.zeros = lambda s: [0]*s; np.clip = lambda v,lo,hi: max(lo,min(hi,v)); np.argmax = lambda a: a.index(max(a)); sys.modules['numpy'] = np
import ast
src = open('evcc-smartload/rootfs/app/comparator.py').read()
tree = ast.parse(src)
classes = [n.name for n in ast.walk(tree) if isinstance(n, ast.ClassDef)]
assert 'Comparator' in classes
methods = []
for node in ast.walk(tree):
    if isinstance(node, ast.FunctionDef):
        methods.append(node.name)
assert 'compare_residual' in methods, 'Missing compare_residual method'
assert 'get_recent_comparisons' in methods, 'Missing get_recent_comparisons method'
assert 'cumulative_savings_eur' in methods, 'Missing cumulative_savings_eur method'
assert 'avg_daily_savings' in methods, 'Missing avg_daily_savings method'
assert 'compare' in methods, 'Existing compare method must be preserved'
assert 'get_status' in methods, 'Existing get_status method must be preserved'
print('PASS: Comparator extension verified')
"
    </automated>
    <manual>Verify comparator.py has both old and new comparison methods</manual>
  </verify>
  <done>Comparator has compare_residual() using slot-0 costs, get_recent_comparisons() with rolling window, cumulative_savings_eur(), avg_daily_savings(), all backward-compatible with existing compare() and get_status()</done>
</task>

</tasks>

<verification>
1. ResidualRLAgent class exists with N_ACTIONS=49, select_delta(), apply_correction(), calculate_reward()
2. StratifiedReplayBuffer class exists with 4 seasonal sub-buffers
3. Model version 2 migration guard detects old Q-table format
4. Shadow/advisory mode attribute with persistence
5. run_constraint_audit() evaluates 4 safety checks against shadow corrections
6. get_audit_result() returns cached audit; maybe_promote() transitions shadow->advisory
7. Comparator.compare_residual() uses slot-0 cost, not full-horizon LP objective
6. Existing Comparator.compare() and get_status() still work unchanged
7. No changes to state.py or state vector size
</verification>

<success_criteria>
- ResidualRLAgent replaces DQNAgent as the active RL class with 49 delta-action space
- Stratified replay buffer prevents seasonal forgetting with per-season deques
- Shadow corrections are logged for later constraint audit
- Comparator extended with slot-0 cost comparison and rolling 7-day windows
- All existing Comparator functionality preserved for backward compatibility
</success_criteria>

<output>
After completion, create `.planning/phases/08-residual-rl-and-learning/08-01-SUMMARY.md`
</output>
